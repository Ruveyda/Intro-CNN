{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruveyda/Intro-CNN/blob/master/IntroCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDAay5yc07eR",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey5AHHOtNhmL",
        "colab_type": "text"
      },
      "source": [
        "**1. Neurons :** A neuron takes inputs, does some calculation with them, and produces one output.\n",
        "\n",
        "3 things are happening here :\n",
        "\n",
        "\n",
        "\n",
        "*   First, each input is multiplied by a weight\n",
        "        \n",
        "        x1 -> x1 * w1\n",
        "        x2 -> x2 * w2\n",
        "\n",
        "*   Second, all the weighted inputs are added together with a bias :\n",
        "\n",
        "        (x1 * w1) + (x2 * w2) + b\n",
        "       \n",
        "       \n",
        "\n",
        "*  Finally, the sum is passed through an activation function\n",
        "\n",
        "        y = f(x1 * w1 + x2*w2 + b)\n",
        "       \n",
        "In here, our activation function is sigmoid function :\n",
        "\n",
        "        f(x) = 1 / (1 + e^(-x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E2LLz7vP4-x",
        "colab_type": "text"
      },
      "source": [
        "- **Coding a Neuron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buOmJg_oQDYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qso3iQXmQIpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+ np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOUoRQlmQS-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Neuron:\n",
        "  \n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "   \n",
        "  def feedforward(self, inputs):\n",
        "    total = np.dot(self.weights, inputs) + bias\n",
        "    return sigmoid(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tk4h-DNRnfx",
        "colab_type": "code",
        "outputId": "5b23dbd0-9293-459d-f701-f3fe8d384bd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weights = np.array([0, 1])\n",
        "bias = 4\n",
        "\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])\n",
        "\n",
        "print(n.feedforward(x))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9990889488055994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdoMMF4ISbVX",
        "colab_type": "text"
      },
      "source": [
        "**2. Combining Neurons into a Neural Networks**\n",
        "\n",
        "A neural network is a bunch of neurons connected together.\n",
        "\n",
        "A hidden layer is any layer between the input(first) layer and output(last) layer. There can be multiple hidden layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1YspbtM0AqF",
        "colab_type": "text"
      },
      "source": [
        "- **Coding a Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_puc8401ZS6",
        "colab_type": "text"
      },
      "source": [
        "A neural network with\n",
        "- 2 inputs, \n",
        "- a hidden layer with 2 neurons(h1, h2), \n",
        "- an output layer with 1 neuron(o1).\n",
        "\n",
        "Each neuron has the same weights and bias\n",
        "- w = [0,1]\n",
        "- b = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITMisLas0Teh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqRIzYSQ1S45",
        "colab_type": "code",
        "outputId": "4686f29e-ed1d-4200-d609-c2ec2184621c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NeuralNetwork:\n",
        "  \n",
        "  def __init__(self):\n",
        "    weights = np.array([0, 1])\n",
        "    bias = 0\n",
        "    \n",
        "    self.h1 = Neuron(weights, bias)\n",
        "    self.h2 = Neuron(weights, bias)\n",
        "    self.o1 = Neuron(weights, bias)\n",
        "    \n",
        "  def feedforward(self, x):\n",
        "    out_h1 = self.h1.feedforward(x)\n",
        "    out_h2 = self.h2.feedforward(x)\n",
        "    \n",
        "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "    \n",
        "    return out_o1\n",
        "  \n",
        "network = NeuralNetwork()\n",
        "x = np.array([2, 3])\n",
        "print(network.feedforward(x))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9933010896328802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09wGG3tDQWU",
        "colab_type": "text"
      },
      "source": [
        "**3. Training a Neural Network**\n",
        "\n",
        "Say we have the following measurements:\n",
        "\n",
        "          Weight     Height     Gender\n",
        "          \n",
        "Alice =  ---------- 133 ---------------- 65 ---------------- F\n",
        "\n",
        "Bob = ---------- 160 ---------------- 72 ---------------- M\n",
        "\n",
        "Charlie = ---------- 152 ---------------- 70 ---------------- M\n",
        "\n",
        "Diana =---------- 120 ---------------- 60 ---------------- F\n",
        "\n",
        "\n",
        "\n",
        "Let's train our network to predict someone's gender given their weight and height:\n",
        "\n",
        "- Input Layer : weight, height\n",
        "- Hidden Layer : h1, h2\n",
        "- Output Layer : gender\n",
        "\n",
        "We'll represent Male with a 0 and Female with a 1, and we'll also shift the data it easier to use :\n",
        "\n",
        "\n",
        "          Weight (-135)     Height(-66)     Gender\n",
        "          \n",
        "Alice =  ------------- -2 ------------------------------------- -1 ---------------- 1\n",
        "\n",
        "Bob = ------------- 25 ------------------------------------- 6 ---------------- 0\n",
        "\n",
        "Charlie = ------------- 17 ------------------------------------- 4 ---------------- 0\n",
        "\n",
        "Diana =------------- -15 ------------------------------------- -6 ---------------- 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDWyfqusGazO",
        "colab_type": "text"
      },
      "source": [
        "**Loss**\n",
        "\n",
        "Before we train our network, we first need a way to quantify how \"good\" it's doing so that it can try to do \"better\". That's what the loss is.\n",
        "\n",
        "We'll use the mean squared error (MSE) loss:\n",
        "\n",
        "MSE = 1/n âˆ‘ (Ytrue - Ypred)^2 \n",
        "\n",
        "      (i=1 -> n)\n",
        "\n",
        "- n is the number of samples, which is 4 (Alice, Bob, Charlie, Diana)\n",
        "- y presents the variable being predicted, which is Gender.\n",
        "- Ytrue is the true value of the variable (the correct answer). For example, Ytrue for Alice would be 1. (Female)\n",
        "- Ypred is the predicted value of the variable. It's whatever our newtwork outputs.\n",
        "\n",
        "(Ytrue - Ypred)^2 is known as the squared error. Our loss function is simply taking the average over all squared errors(hence the name mean squared error). The better our predictions are, the lower our loss will be.\n",
        "\n",
        "Better predictions = Lower Loss.\n",
        "\n",
        "***Training a network = trying to minimize its loss***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt99Ep7yOpXx",
        "colab_type": "text"
      },
      "source": [
        "- **Coding MSE Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYmaa4FIOuDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyR3cwX5Oxqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "  return((y_true - y_pred)**2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf7p2zRzPBDT",
        "colab_type": "code",
        "outputId": "a307c318-545c-4701-ec91-44a4cafbfff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_true = np.array([1,0,0,1])\n",
        "y_pred = np.array([0,0,0,0])\n",
        "\n",
        "print(mse_loss(y_true, y_pred))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0TkVTxxPYaq",
        "colab_type": "text"
      },
      "source": [
        "**4. Training a Neural Network - Minimize the Loss**\n",
        "\n",
        "We know have a clear goal : minimize the loss of the neural network. We know we can change the network's weights and biases to influance its predictions, but how do we do so in a way that decreases loss?\n",
        "\n",
        "For simplicity, let's pretend we only have Alice in our dataset:\n",
        "\n",
        "          Weight (-135)     Height(-66)     Gender\n",
        "          \n",
        "Alice =  ------------- -2 ------------------------------------- -1 ---------------- 1\n",
        "\n",
        "\n",
        "Then the mean squared error loss is just Alice's squared error:\n",
        "\n",
        "    MSE = 1/1 âˆ‘ (Ytrue - Ypred)^2 \n",
        "           (i=1 -> 1)\n",
        "        = (Ytrue - Ypred) ^ 2\n",
        "        = (1- Ypred)^2\n",
        "        \n",
        "Another way to think about loss is a function of weights and biases. Let's label each weight and bias in our network.\n",
        "\n",
        "We can write loss as a multivariable fuction:\n",
        "\n",
        "L(w1,w2,w3,w4,w5,w6, b1,b2,b3)\n",
        "\n",
        "How would loss L change if we changed w1? That's a question the partial derivative âˆ‚L/âˆ‚w1 can answer.\n",
        "\n",
        "âˆ‚w1 / âˆ‚L = (âˆ‚ypred / âˆ‚L) âˆ— (âˆ‚w1 /âˆ‚ypred)\n",
        "\n",
        "This system of calculating partial derivatives by working backwards is known as **backpropagation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d4hnLuvZuNs",
        "colab_type": "text"
      },
      "source": [
        "**Training : Stochastic Gradient Descent**\n",
        "\n",
        "We'll use an optimization algorithm called atochastic gradient descent that tells us how to change our weights and biases to minimize loss. \n",
        "\n",
        "\n",
        "w1 <- w1 - Î· ( âˆ‚L/âˆ‚w1 )\n",
        "\n",
        "Î· is constant called the learning rate that controls how fast we train. All we're doing is subtracting Î· âˆ‚L1 / âˆ‚w1 from w1 :\n",
        "\n",
        "- If âˆ‚L1 / âˆ‚w1 is positive, w1 will decrease, which makes L decrease.\n",
        "- If âˆ‚L1 / âˆ‚w1 is negative, w1 will increase, which makes L decrease.\n",
        "\n",
        "If we do this for every weight and bias in the network, the loss will slowly decrease and our network will improve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6zKpH5dkBqt",
        "colab_type": "text"
      },
      "source": [
        "**Code : A Complete Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmpv0zNakHkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0XKruhRkLUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+ np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zang82-jkWlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deriv_sigmoid(x):\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1-fx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zd7YLaTkqRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse_loss(y_true, y_pred):\n",
        "  return((y_true - y_pred)**2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stDzuLilk3Nq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "176390ad-08ad-45c4-850e-cc56882a4bbd"
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self):\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "    \n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "    self.b3 = np.random.normal()\n",
        "    \n",
        "  def feedforward(self, x):\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "    return o1\n",
        "  \n",
        "  def train(self, data, all_y_trues):\n",
        "    learn_rate = 0.1\n",
        "    epochs = 1000\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        \n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "        h1 = sigmoid(sum_h1)\n",
        "        \n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "        h2 = sigmoid(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "        o1 = sigmoid(sum_o1)\n",
        "        \n",
        "        y_pred = o1\n",
        "        \n",
        "        # --- Calculate partial derivatives.\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neuron o1\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "        # Neuron h1\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "        # Neuron h2\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "        # --- Update weights and biases\n",
        "        # Neuron h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neuron h2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neuron o1\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "        \n",
        "  # --- Calculate total loss at the end of each epoch\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "        loss = mse_loss(all_y_trues, y_preds)\n",
        "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
        "       \n",
        "      \n",
        "      \n",
        "data = np.array([\n",
        "    [-2,-1],\n",
        "    [25,6],\n",
        "    [17,4],\n",
        "    [-15,-6]\n",
        "])\n",
        "\n",
        "all_y_trues = np.array([\n",
        "    1,\n",
        "    0,\n",
        "    0,\n",
        "    1\n",
        "])\n",
        "\n",
        "network = NeuralNetwork()\n",
        "network.train(data, all_y_trues)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 0.228\n",
            "Epoch 10 loss: 0.123\n",
            "Epoch 20 loss: 0.081\n",
            "Epoch 30 loss: 0.060\n",
            "Epoch 40 loss: 0.047\n",
            "Epoch 50 loss: 0.039\n",
            "Epoch 60 loss: 0.032\n",
            "Epoch 70 loss: 0.028\n",
            "Epoch 80 loss: 0.024\n",
            "Epoch 90 loss: 0.021\n",
            "Epoch 100 loss: 0.019\n",
            "Epoch 110 loss: 0.017\n",
            "Epoch 120 loss: 0.016\n",
            "Epoch 130 loss: 0.014\n",
            "Epoch 140 loss: 0.013\n",
            "Epoch 150 loss: 0.012\n",
            "Epoch 160 loss: 0.011\n",
            "Epoch 170 loss: 0.011\n",
            "Epoch 180 loss: 0.010\n",
            "Epoch 190 loss: 0.009\n",
            "Epoch 200 loss: 0.009\n",
            "Epoch 210 loss: 0.008\n",
            "Epoch 220 loss: 0.008\n",
            "Epoch 230 loss: 0.008\n",
            "Epoch 240 loss: 0.007\n",
            "Epoch 250 loss: 0.007\n",
            "Epoch 260 loss: 0.007\n",
            "Epoch 270 loss: 0.006\n",
            "Epoch 280 loss: 0.006\n",
            "Epoch 290 loss: 0.006\n",
            "Epoch 300 loss: 0.006\n",
            "Epoch 310 loss: 0.005\n",
            "Epoch 320 loss: 0.005\n",
            "Epoch 330 loss: 0.005\n",
            "Epoch 340 loss: 0.005\n",
            "Epoch 350 loss: 0.005\n",
            "Epoch 360 loss: 0.005\n",
            "Epoch 370 loss: 0.004\n",
            "Epoch 380 loss: 0.004\n",
            "Epoch 390 loss: 0.004\n",
            "Epoch 400 loss: 0.004\n",
            "Epoch 410 loss: 0.004\n",
            "Epoch 420 loss: 0.004\n",
            "Epoch 430 loss: 0.004\n",
            "Epoch 440 loss: 0.004\n",
            "Epoch 450 loss: 0.004\n",
            "Epoch 460 loss: 0.003\n",
            "Epoch 470 loss: 0.003\n",
            "Epoch 480 loss: 0.003\n",
            "Epoch 490 loss: 0.003\n",
            "Epoch 500 loss: 0.003\n",
            "Epoch 510 loss: 0.003\n",
            "Epoch 520 loss: 0.003\n",
            "Epoch 530 loss: 0.003\n",
            "Epoch 540 loss: 0.003\n",
            "Epoch 550 loss: 0.003\n",
            "Epoch 560 loss: 0.003\n",
            "Epoch 570 loss: 0.003\n",
            "Epoch 580 loss: 0.003\n",
            "Epoch 590 loss: 0.003\n",
            "Epoch 600 loss: 0.003\n",
            "Epoch 610 loss: 0.003\n",
            "Epoch 620 loss: 0.002\n",
            "Epoch 630 loss: 0.002\n",
            "Epoch 640 loss: 0.002\n",
            "Epoch 650 loss: 0.002\n",
            "Epoch 660 loss: 0.002\n",
            "Epoch 670 loss: 0.002\n",
            "Epoch 680 loss: 0.002\n",
            "Epoch 690 loss: 0.002\n",
            "Epoch 700 loss: 0.002\n",
            "Epoch 710 loss: 0.002\n",
            "Epoch 720 loss: 0.002\n",
            "Epoch 730 loss: 0.002\n",
            "Epoch 740 loss: 0.002\n",
            "Epoch 750 loss: 0.002\n",
            "Epoch 760 loss: 0.002\n",
            "Epoch 770 loss: 0.002\n",
            "Epoch 780 loss: 0.002\n",
            "Epoch 790 loss: 0.002\n",
            "Epoch 800 loss: 0.002\n",
            "Epoch 810 loss: 0.002\n",
            "Epoch 820 loss: 0.002\n",
            "Epoch 830 loss: 0.002\n",
            "Epoch 840 loss: 0.002\n",
            "Epoch 850 loss: 0.002\n",
            "Epoch 860 loss: 0.002\n",
            "Epoch 870 loss: 0.002\n",
            "Epoch 880 loss: 0.002\n",
            "Epoch 890 loss: 0.002\n",
            "Epoch 900 loss: 0.002\n",
            "Epoch 910 loss: 0.002\n",
            "Epoch 920 loss: 0.002\n",
            "Epoch 930 loss: 0.002\n",
            "Epoch 940 loss: 0.002\n",
            "Epoch 950 loss: 0.002\n",
            "Epoch 960 loss: 0.002\n",
            "Epoch 970 loss: 0.002\n",
            "Epoch 980 loss: 0.002\n",
            "Epoch 990 loss: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "076aP7jVThBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9490503b-c820-4ea2-c15d-72b8a7d8929d"
      },
      "source": [
        "emily = np.array([-7, -3])\n",
        "frank = np.array([20,2])\n",
        "\n",
        "print(\"Emily: %.3f\" % network.feedforward(emily))\n",
        "print(\"Frank: %.3f\" % network.feedforward(frank))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Emily: 0.966\n",
            "Frank: 0.039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTuG-zNqezza",
        "colab_type": "text"
      },
      "source": [
        "Emily : Female\n",
        "Frank : Male"
      ]
    }
  ]
}
